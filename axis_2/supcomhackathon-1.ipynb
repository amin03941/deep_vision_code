{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12269330,
          "sourceType": "datasetVersion",
          "datasetId": 7731587
        },
        {
          "sourceId": 12634079,
          "sourceType": "datasetVersion",
          "datasetId": 7725006
        },
        {
          "sourceId": 13081338,
          "sourceType": "datasetVersion",
          "datasetId": 8285142,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 14551983,
          "sourceType": "datasetVersion",
          "datasetId": 9294610
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "supcomhackathon",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"malloulifares/d2d-cytokine-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "x8_ErfLNIjEA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"noob786/mpeg-g-microbiomeclassificationconvertedfastqfiles\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bR1Hhht3IjEA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"noob786/secondbatchoffastqfiles\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OtPuQk-rIjEA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y && apt-get install -y kmc\n",
        "!pip install transformers torch pandas numpy tqdm qdrant-client"
      ],
      "metadata": {
        "trusted": true,
        "id": "kx1BMIitIjEB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y triton"
      ],
      "metadata": {
        "trusted": true,
        "id": "xmuWi76CIjEB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] CRITICAL FIX: Uninstall Triton to avoid trans_b error\n",
        "# Run this in a code cell BEFORE importing anything:\n",
        "# !pip uninstall -y triton\n",
        "\n",
        "# [2] IMPORTS\n",
        "import torch\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import uuid\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# [3] CONFIGURATION (Matches your provided paths)\n",
        "class Config:\n",
        "    TRAIN_CSV = \"/kaggle/input/trainmpeg/Train.csv\"\n",
        "    SUBJECT_CSV = \"/kaggle/input/trainmpeg/Train_Subjects.csv\"\n",
        "    CYTOKINE_CSV = \"/kaggle/input/d2d-cytokine-data/cytokine_profiles.csv\"\n",
        "\n",
        "    # Where the raw genomic sequences live\n",
        "    FASTQ_DIRS = [\n",
        "        \"/kaggle/input/mpeg-g-microbiomeclassificationconvertedfastqfiles/TrainFiles/TrainFiles\",\n",
        "        \"/kaggle/input/secondbatchoffastqfiles/TrainFiles\"\n",
        "    ]\n",
        "\n",
        "    # Models & params\n",
        "    DENSE_MODEL_ID = \"zhihan1996/DNABERT-2-117M\"\n",
        "    OUTPUT_FILENAME = \"bio_memory_dump.pkl\"\n",
        "    KMER_SIZE = 6\n",
        "    MAX_DNA_LEN = 512 # Truncate sequences for DNABERT efficiency\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚öôÔ∏è Hardware Acceleration: {device}\")\n",
        "\n",
        "# [4] DATA INGESTION LOGIC\n",
        "def load_metadata():\n",
        "    print(\"üìÇ Loading & Merging Metadata...\")\n",
        "    train = pd.read_csv(Config.TRAIN_CSV)\n",
        "    subj = pd.read_csv(Config.SUBJECT_CSV)\n",
        "    cyto = pd.read_csv(Config.CYTOKINE_CSV)\n",
        "\n",
        "    # Merge all metadata into one view\n",
        "    df = pd.merge(train, cyto, on=\"SampleID\", how=\"inner\")\n",
        "    df = pd.merge(df, subj, on=\"SubjectID\", how=\"left\")\n",
        "\n",
        "    # Helper to find actual file paths on Kaggle disk\n",
        "    def get_path(fname):\n",
        "        base = fname.replace('.mgb', '.fastq')\n",
        "        for d in Config.FASTQ_DIRS:\n",
        "            for ext in ['', '.gz']:\n",
        "                p = os.path.join(d, base + ext)\n",
        "                if os.path.exists(p): return p\n",
        "        return None\n",
        "\n",
        "    print(\"   Mapping FASTQ paths...\")\n",
        "    df['filepath'] = df['filename'].apply(get_path)\n",
        "    df_clean = df.dropna(subset=['filepath']).reset_index(drop=True)\n",
        "    print(f\"‚úÖ Ready to process {len(df_clean)} samples.\")\n",
        "    return df_clean\n",
        "\n",
        "# [5] VECTORIZATION ENGINES\n",
        "print(\"üß† Loading DNABERT-2 (Genomic Foundation Model)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True).to(device)\n",
        "\n",
        "def generate_dense_embedding(file_path):\n",
        "    \"\"\"\n",
        "    Reads DNA sequence -> DNABERT-2 -> Semantic Vector (768d)\n",
        "    \"\"\"\n",
        "    sequence_snippet = \"\"\n",
        "    try:\n",
        "        # Read file, skipping header, taking the first sequence line\n",
        "        with open(file_path, 'r') as f:\n",
        "            next(f)\n",
        "            sequence_snippet = next(f).strip()[:Config.MAX_DNA_LEN]\n",
        "    except:\n",
        "        return np.zeros(768).tolist() # Fail safe\n",
        "\n",
        "    inputs = tokenizer(sequence_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=Config.MAX_DNA_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Mean pooling to represent whole sequence chunk\n",
        "        embedding = torch.mean(outputs[0], dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return embedding.tolist()\n",
        "\n",
        "def generate_sparse_embedding(file_path):\n",
        "    \"\"\"\n",
        "    Reads DNA -> KMC Count -> Sparse Vector (Indices, Values)\n",
        "    This captures exact microbial motifs.\n",
        "    \"\"\"\n",
        "    tmp_uuid = str(uuid.uuid4())\n",
        "    tmp_prefix = f\"/tmp/{tmp_uuid}\"\n",
        "\n",
        "    # 1. Run KMC (Fast C++ k-mer counter)\n",
        "    cmd = f\"kmc -k{Config.KMER_SIZE} -ci1 -fm {file_path} {tmp_prefix} /tmp\"\n",
        "    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 2. Dump KMC database to readable text\n",
        "    subprocess.run(f\"kmc_tools transform {tmp_prefix} dump {tmp_prefix}.txt\",\n",
        "                   shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    # 3. Parse text to Sparse Format\n",
        "    if os.path.exists(f\"{tmp_prefix}.txt\"):\n",
        "        with open(f\"{tmp_prefix}.txt\", 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    kmer_seq, count = parts[0], parts[1]\n",
        "                    # Hash string k-mer to integer index for vector db\n",
        "                    # This maps 'ATCGCG' -> 12345\n",
        "                    idx = hash(kmer_seq) % 100000\n",
        "                    indices.append(idx)\n",
        "                    values.append(int(count))\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(f\"{tmp_prefix}.txt\")\n",
        "        for ext in ['.kmc_pre', '.kmc_suf']:\n",
        "            if os.path.exists(tmp_prefix + ext): os.remove(tmp_prefix + ext)\n",
        "\n",
        "    return indices, values\n",
        "\n",
        "# [6] EXECUTION PIPELINE\n",
        "df = load_metadata()\n",
        "\n",
        "# === OPTIONAL: Limit rows for testing speed ===\n",
        "# Remove this line for full processing\n",
        "df = df.head(50)\n",
        "\n",
        "vectors_payload = []\n",
        "\n",
        "print(f\"üöÄ Starting Hybrid Vectorization on {len(df)} samples...\")\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "\n",
        "    # Generate Vectors\n",
        "    dense = generate_dense_embedding(row['filepath'])\n",
        "    sp_idx, sp_val = generate_sparse_embedding(row['filepath'])\n",
        "\n",
        "    # Structure data for Qdrant\n",
        "    item = {\n",
        "        \"id\": idx,\n",
        "        \"vector\": {\n",
        "            \"dense\": dense,\n",
        "            \"sparse\": {\"indices\": sp_idx, \"values\": sp_val}\n",
        "        },\n",
        "        \"payload\": {\n",
        "            \"SampleID\": row['SampleID'],\n",
        "            \"BodySite\": row.get('SampleType', 'Unknown'),\n",
        "            # Metabolic Targets for Analysis\n",
        "            \"IL22\": row.get('IL22', 0),\n",
        "            \"EGF\": row.get('EGF', 0),\n",
        "            \"TNFA\": row.get('TNFA', 0),\n",
        "            \"InsulinSensitivity\": row.get('Insulin_Sensitivity_Label', 'Unknown')\n",
        "        }\n",
        "    }\n",
        "    vectors_payload.append(item)\n",
        "\n",
        "# [7] EXPORT\n",
        "print(f\"üíæ Saving Bio-Memory to {Config.OUTPUT_FILENAME}...\")\n",
        "with open(Config.OUTPUT_FILENAME, 'wb') as f:\n",
        "    pickle.dump(vectors_payload, f)\n",
        "\n",
        "print(\"‚úÖ Done! Check the 'Output' tab to download your .pkl file.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-19T20:38:14.66811Z",
          "iopub.execute_input": "2026-01-19T20:38:14.668925Z",
          "iopub.status.idle": "2026-01-19T20:38:48.031755Z",
          "shell.execute_reply.started": "2026-01-19T20:38:14.668889Z",
          "shell.execute_reply": "2026-01-19T20:38:48.03095Z"
        },
        "id": "oP_-oY-hIjEB",
        "outputId": "d91c2ab6-9d61-4da0-85c9-31573c873077"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚öôÔ∏è Hardware Acceleration: cuda\nüß† Loading DNABERT-2 (Genomic Foundation Model)...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2026-01-19 20:38:20.729718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768855100.751637     943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768855100.758448     943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768855100.775645     943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768855100.775664     943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768855100.775666     943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768855100.775669     943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT_hyphen_2_hyphen_117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üìÇ Loading & Merging Metadata...\n   Mapping FASTQ paths...\n‚úÖ Ready to process 1982 samples.\nüöÄ Starting Hybrid Vectorization on 50 samples...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:21<00:00,  2.31it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üíæ Saving Bio-Memory to bio_memory_dump.pkl...\n‚úÖ Done! Check the 'Output' tab to download your .pkl file.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] CRITICAL FIX: Uninstall Triton to avoid trans_b error\n",
        "# Run this in a code cell BEFORE importing anything:\n",
        "# !pip uninstall -y triton\n",
        "\n",
        "# [2] IMPORTS\n",
        "import torch\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import uuid\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# [3] CONFIGURATION (Matches your provided paths)\n",
        "class Config:\n",
        "    TRAIN_CSV = \"/kaggle/input/trainmpeg/Train.csv\"\n",
        "    SUBJECT_CSV = \"/kaggle/input/trainmpeg/Train_Subjects.csv\"\n",
        "    CYTOKINE_CSV = \"/kaggle/input/d2d-cytokine-data/cytokine_profiles.csv\"\n",
        "\n",
        "    # Where the raw genomic sequences live\n",
        "    FASTQ_DIRS = [\n",
        "        \"/kaggle/input/mpeg-g-microbiomeclassificationconvertedfastqfiles/TrainFiles/TrainFiles\",\n",
        "        \"/kaggle/input/secondbatchoffastqfiles/TrainFiles\"\n",
        "    ]\n",
        "\n",
        "    # Models & params\n",
        "    DENSE_MODEL_ID = \"zhihan1996/DNABERT-2-117M\"\n",
        "    OUTPUT_FILENAME = \"bio_memory_dump.pkl\"\n",
        "    KMER_SIZE = 6\n",
        "    MAX_DNA_LEN = 512 # Truncate sequences for DNABERT efficiency\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚öôÔ∏è Hardware Acceleration: {device}\")\n",
        "\n",
        "# [4] DATA INGESTION LOGIC\n",
        "def load_metadata():\n",
        "    print(\"üìÇ Loading & Merging Metadata...\")\n",
        "    train = pd.read_csv(Config.TRAIN_CSV)\n",
        "    subj = pd.read_csv(Config.SUBJECT_CSV)\n",
        "    cyto = pd.read_csv(Config.CYTOKINE_CSV)\n",
        "\n",
        "    # Merge all metadata into one view\n",
        "    df = pd.merge(train, cyto, on=\"SampleID\", how=\"inner\")\n",
        "    df = pd.merge(df, subj, on=\"SubjectID\", how=\"left\")\n",
        "\n",
        "    # Helper to find actual file paths on Kaggle disk\n",
        "    def get_path(fname):\n",
        "        base = fname.replace('.mgb', '.fastq')\n",
        "        for d in Config.FASTQ_DIRS:\n",
        "            for ext in ['', '.gz']:\n",
        "                p = os.path.join(d, base + ext)\n",
        "                if os.path.exists(p): return p\n",
        "        return None\n",
        "\n",
        "    print(\"   Mapping FASTQ paths...\")\n",
        "    df['filepath'] = df['filename'].apply(get_path)\n",
        "    df_clean = df.dropna(subset=['filepath']).reset_index(drop=True)\n",
        "    print(f\"‚úÖ Ready to process {len(df_clean)} samples.\")\n",
        "    return df_clean\n",
        "\n",
        "# [5] VECTORIZATION ENGINES\n",
        "print(\"üß† Loading DNABERT-2 (Genomic Foundation Model)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True).to(device)\n",
        "\n",
        "def generate_dense_embedding(file_path):\n",
        "    \"\"\"\n",
        "    Reads DNA sequence -> DNABERT-2 -> Semantic Vector (768d)\n",
        "    \"\"\"\n",
        "    sequence_snippet = \"\"\n",
        "    try:\n",
        "        # Read file, skipping header, taking the first sequence line\n",
        "        with open(file_path, 'r') as f:\n",
        "            next(f)\n",
        "            sequence_snippet = next(f).strip()[:Config.MAX_DNA_LEN]\n",
        "    except:\n",
        "        return np.zeros(768).tolist() # Fail safe\n",
        "\n",
        "    inputs = tokenizer(sequence_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=Config.MAX_DNA_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Mean pooling to represent whole sequence chunk\n",
        "        embedding = torch.mean(outputs[0], dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return embedding.tolist()\n",
        "\n",
        "def generate_sparse_embedding(file_path):\n",
        "    \"\"\"\n",
        "    Reads DNA -> KMC Count -> Sparse Vector (Indices, Values)\n",
        "    This captures exact microbial motifs.\n",
        "    \"\"\"\n",
        "    tmp_uuid = str(uuid.uuid4())\n",
        "    tmp_prefix = f\"/tmp/{tmp_uuid}\"\n",
        "\n",
        "    # 1. Run KMC (Fast C++ k-mer counter)\n",
        "    cmd = f\"kmc -k{Config.KMER_SIZE} -ci1 -fm {file_path} {tmp_prefix} /tmp\"\n",
        "    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 2. Dump KMC database to readable text\n",
        "    subprocess.run(f\"kmc_tools transform {tmp_prefix} dump {tmp_prefix}.txt\",\n",
        "                   shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    # 3. Parse text to Sparse Format\n",
        "    if os.path.exists(f\"{tmp_prefix}.txt\"):\n",
        "        with open(f\"{tmp_prefix}.txt\", 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    kmer_seq, count = parts[0], parts[1]\n",
        "                    # Hash string k-mer to integer index for vector db\n",
        "                    # This maps 'ATCGCG' -> 12345\n",
        "                    idx = hash(kmer_seq) % 100000\n",
        "                    indices.append(idx)\n",
        "                    values.append(int(count))\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(f\"{tmp_prefix}.txt\")\n",
        "        for ext in ['.kmc_pre', '.kmc_suf']:\n",
        "            if os.path.exists(tmp_prefix + ext): os.remove(tmp_prefix + ext)\n",
        "\n",
        "    return indices, values\n",
        "\n",
        "# [6] EXECUTION PIPELINE\n",
        "df = load_metadata()\n",
        "\n",
        "# Processing full dataset (all 1982 samples)\n",
        "print(f\"üìä Processing complete dataset: {len(df)} samples\")\n",
        "\n",
        "vectors_payload = []\n",
        "\n",
        "print(f\"üöÄ Starting Hybrid Vectorization on {len(df)} samples...\")\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "\n",
        "    # Generate Vectors\n",
        "    dense = generate_dense_embedding(row['filepath'])\n",
        "    sp_idx, sp_val = generate_sparse_embedding(row['filepath'])\n",
        "\n",
        "    # Structure data for Qdrant\n",
        "    item = {\n",
        "        \"id\": idx,\n",
        "        \"vector\": {\n",
        "            \"dense\": dense,\n",
        "            \"sparse\": {\"indices\": sp_idx, \"values\": sp_val}\n",
        "        },\n",
        "        \"payload\": {\n",
        "            \"SampleID\": row['SampleID'],\n",
        "            \"BodySite\": row.get('SampleType', 'Unknown'),\n",
        "            # Metabolic Targets for Analysis\n",
        "            \"IL22\": row.get('IL22', 0),\n",
        "            \"EGF\": row.get('EGF', 0),\n",
        "            \"TNFA\": row.get('TNFA', 0),\n",
        "            \"InsulinSensitivity\": row.get('Insulin_Sensitivity_Label', 'Unknown')\n",
        "        }\n",
        "    }\n",
        "    vectors_payload.append(item)\n",
        "\n",
        "# [7] EXPORT\n",
        "print(f\"üíæ Saving Bio-Memory to {Config.OUTPUT_FILENAME}...\")\n",
        "with open(Config.OUTPUT_FILENAME, 'wb') as f:\n",
        "    pickle.dump(vectors_payload, f)\n",
        "\n",
        "print(\"‚úÖ Done! Check the 'Output' tab to download your .pkl file.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-19T20:39:38.758164Z",
          "iopub.execute_input": "2026-01-19T20:39:38.758564Z",
          "iopub.status.idle": "2026-01-19T20:56:27.529272Z",
          "shell.execute_reply.started": "2026-01-19T20:39:38.758533Z",
          "shell.execute_reply": "2026-01-19T20:56:27.528535Z"
        },
        "id": "cx97GtqfIjED",
        "outputId": "8f83fffc-ea1c-4c1e-8a42-43a8c038d18f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚öôÔ∏è Hardware Acceleration: cuda\nüß† Loading DNABERT-2 (Genomic Foundation Model)...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT_hyphen_2_hyphen_117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üìÇ Loading & Merging Metadata...\n   Mapping FASTQ paths...\n‚úÖ Ready to process 1982 samples.\nüìä Processing complete dataset: 1982 samples\nüöÄ Starting Hybrid Vectorization on 1982 samples...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982/1982 [16:45<00:00,  1.97it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üíæ Saving Bio-Memory to bio_memory_dump.pkl...\n‚úÖ Done! Check the 'Output' tab to download your .pkl file.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- PATHS ---\n",
        "# Adjust these if your input directory structure is different\n",
        "TRAIN_CSV = \"/kaggle/input/trainmpeg/Train.csv\"\n",
        "SUBJECT_CSV = \"/kaggle/input/trainmpeg/Train_Subjects.csv\"\n",
        "CYTOKINE_CSV = \"/kaggle/input/d2d-cytokine-data/cytokine_profiles.csv\"\n",
        "\n",
        "def inspect_dataframe(name, df, key_col):\n",
        "    print(f\"\\n{'='*20} ANALYZING: {name} {'='*20}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Key Column: '{key_col}'\")\n",
        "\n",
        "    # Check Key Type\n",
        "    key_dtype = df[key_col].dtype\n",
        "    print(f\"Key Data Type: {key_dtype}\")\n",
        "\n",
        "    # Check for Whitespace issues if string\n",
        "    if df[key_col].dtype == 'object':\n",
        "        has_whitespace = df[key_col].str.contains(r'\\s', regex=True).any()\n",
        "        print(f\"Contains Whitespace? {has_whitespace}\")\n",
        "\n",
        "        # Show example IDs\n",
        "        print(f\"Example IDs: {df[key_col].head(3).tolist()}\")\n",
        "    else:\n",
        "        print(f\"Example IDs: {df[key_col].head(3).tolist()}\")\n",
        "\n",
        "    # Check Uniqueness\n",
        "    n_unique = df[key_col].nunique()\n",
        "    print(f\"Unique Keys: {n_unique} (Duplicates: {len(df) - n_unique})\")\n",
        "\n",
        "def verify_merge_compatibility():\n",
        "    print(\"Loading CSVs...\")\n",
        "    try:\n",
        "        df_train = pd.read_csv(TRAIN_CSV)\n",
        "        df_subj = pd.read_csv(SUBJECT_CSV)\n",
        "        df_cyto = pd.read_csv(CYTOKINE_CSV)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå Error: File not found. Check your paths.\\n{e}\")\n",
        "        return\n",
        "\n",
        "    # 1. Inspect Individual Files\n",
        "    inspect_dataframe(\"Train.csv (Samples)\", df_train, \"SampleID\")\n",
        "    inspect_dataframe(\"Cytokine.csv (Targets)\", df_cyto, \"SampleID\")\n",
        "    inspect_dataframe(\"Subjects.csv (Metadata)\", df_subj, \"SubjectID\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"               MERGE COMPATIBILITY CHECK\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 2. Check SampleID Match (Train vs Cytokine)\n",
        "    # They usually join on 'SampleID'\n",
        "    train_samples = set(df_train['SampleID'].astype(str).str.strip())\n",
        "    cyto_samples = set(df_cyto['SampleID'].astype(str).str.strip())\n",
        "\n",
        "    common_samples = train_samples.intersection(cyto_samples)\n",
        "    print(f\"\\n[1] SampleID Merge (Train <-> Cytokine)\")\n",
        "    print(f\"   - Train Samples: {len(train_samples)}\")\n",
        "    print(f\"   - Cytokine Samples: {len(cyto_samples)}\")\n",
        "    print(f\"   - ‚úÖ Common Samples: {len(common_samples)}\")\n",
        "\n",
        "    if len(common_samples) == 0:\n",
        "        print(\"   ‚ö†Ô∏è CRITICAL WARNING: No common SampleIDs found. Check ID formats!\")\n",
        "\n",
        "    # 3. Check SubjectID Match (Train vs Subjects)\n",
        "    # They join on 'SubjectID'\n",
        "    # This is the most common failure point (Int vs String)\n",
        "    print(f\"\\n[2] SubjectID Merge (Train <-> Subjects)\")\n",
        "\n",
        "    # Raw Types\n",
        "    type_train = df_train['SubjectID'].dtype\n",
        "    type_subj = df_subj['SubjectID'].dtype\n",
        "    print(f\"   - Train 'SubjectID' Type: {type_train}\")\n",
        "    print(f\"   - Subj 'SubjectID' Type: {type_subj}\")\n",
        "\n",
        "    if type_train != type_subj:\n",
        "        print(\"   ‚ö†Ô∏è WARNING: Type Mismatch detected! One is Int, one is Object/String.\")\n",
        "        print(\"   -> The pipeline MUST convert both to strings before merging.\")\n",
        "\n",
        "    # Test Overlap (converting to string to simulate the fix)\n",
        "    train_subs = set(df_train['SubjectID'].astype(str).str.strip())\n",
        "    subj_subs = set(df_subj['SubjectID'].astype(str).str.strip())\n",
        "\n",
        "    common_subs = train_subs.intersection(subj_subs)\n",
        "    print(f\"   - Train Subjects: {len(train_subs)}\")\n",
        "    print(f\"   - Metadata Subjects: {len(subj_subs)}\")\n",
        "    print(f\"   - ‚úÖ Common Subjects (after string conversion): {len(common_subs)}\")\n",
        "\n",
        "    if len(common_subs) == 0:\n",
        "        print(\"   ‚ùå ERROR: Even after string conversion, no common Subjects found. Data IDs are fundamentally different.\")\n",
        "    elif len(common_subs) < len(train_subs):\n",
        "        print(f\"   ‚ÑπÔ∏è Note: {len(train_subs) - len(common_subs)} samples in Train do not have matching Subject Metadata.\")\n",
        "\n",
        "verify_merge_compatibility()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-19T21:23:25.91472Z",
          "iopub.execute_input": "2026-01-19T21:23:25.91502Z",
          "iopub.status.idle": "2026-01-19T21:23:25.984001Z",
          "shell.execute_reply.started": "2026-01-19T21:23:25.914994Z",
          "shell.execute_reply": "2026-01-19T21:23:25.983339Z"
        },
        "id": "EZmyiFbUIjEE",
        "outputId": "eff88825-582e-46c6-b686-280f365f70a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading CSVs...\n\n==================== ANALYZING: Train.csv (Samples) ====================\nShape: (2901, 4)\nKey Column: 'SampleID'\nKey Data Type: object\nContains Whitespace? False\nExample IDs: ['Sample_AFTIWE', 'Sample_JQJVNK', 'Sample_YJWGWW']\nUnique Keys: 1262 (Duplicates: 1639)\n\n==================== ANALYZING: Cytokine.csv (Targets) ====================\nShape: (670, 73)\nKey Column: 'SampleID'\nKey Data Type: object\nContains Whitespace? False\nExample IDs: ['Sample_BDRJDQ', 'Sample_ESYUZA', 'Sample_CNKYCP']\nUnique Keys: 670 (Duplicates: 0)\n\n==================== ANALYZING: Subjects.csv (Metadata) ====================\nShape: (66, 17)\nKey Column: 'SubjectID'\nKey Data Type: object\nContains Whitespace? False\nExample IDs: ['Subject_UDAXIH', 'Subject_NHOSIZ', 'Subject_AYZFWN']\nUnique Keys: 66 (Duplicates: 0)\n\n============================================================\n               MERGE COMPATIBILITY CHECK\n============================================================\n\n[1] SampleID Merge (Train <-> Cytokine)\n   - Train Samples: 1262\n   - Cytokine Samples: 670\n   - ‚úÖ Common Samples: 670\n\n[2] SubjectID Merge (Train <-> Subjects)\n   - Train 'SubjectID' Type: object\n   - Subj 'SubjectID' Type: object\n   - Train Subjects: 66\n   - Metadata Subjects: 66\n   - ‚úÖ Common Subjects (after string conversion): 66\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# [2] IMPORTS\n",
        "import torch\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import uuid\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# [3] CONFIGURATION\n",
        "class Config:\n",
        "    # Input Paths\n",
        "    TRAIN_CSV = \"/kaggle/input/trainmpeg/Train.csv\"\n",
        "    SUBJECT_CSV = \"/kaggle/input/trainmpeg/Train_Subjects.csv\"\n",
        "    CYTOKINE_CSV = \"/kaggle/input/d2d-cytokine-data/cytokine_profiles.csv\"\n",
        "\n",
        "    # FASTQ File Locations\n",
        "    FASTQ_DIRS = [\n",
        "        \"/kaggle/input/mpeg-g-microbiomeclassificationconvertedfastqfiles/TrainFiles/TrainFiles\",\n",
        "        \"/kaggle/input/secondbatchoffastqfiles/TrainFiles\"\n",
        "    ]\n",
        "\n",
        "    # Model Settings\n",
        "    DENSE_MODEL_ID = \"zhihan1996/DNABERT-2-117M\"\n",
        "    OUTPUT_FILENAME = \"bio_memory_dump.pkl\"\n",
        "    KMER_SIZE = 6\n",
        "    MAX_DNA_LEN = 512 # Truncate sequences for efficiency\n",
        "\n",
        "# Detect GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚öôÔ∏è Hardware Acceleration: {device}\")\n",
        "\n",
        "# [4] DATA INGESTION\n",
        "def load_metadata():\n",
        "    print(\"üìÇ Loading Metadata...\")\n",
        "    train = pd.read_csv(Config.TRAIN_CSV)\n",
        "    subj = pd.read_csv(Config.SUBJECT_CSV)\n",
        "    cyto = pd.read_csv(Config.CYTOKINE_CSV)\n",
        "\n",
        "    # --- METADATA CLEANING FIX ---\n",
        "    # Ensure IDs match perfectly (trim whitespace and force string)\n",
        "    print(\"   üßπ Normalizing IDs...\")\n",
        "    train['SampleID'] = train['SampleID'].astype(str).str.strip()\n",
        "    cyto['SampleID'] = cyto['SampleID'].astype(str).str.strip()\n",
        "\n",
        "    train['SubjectID'] = train['SubjectID'].astype(str).str.strip()\n",
        "    subj['SubjectID'] = subj['SubjectID'].astype(str).str.strip()\n",
        "\n",
        "    # Merge: Sample -> Cytokine -> Subject\n",
        "    df = pd.merge(train, cyto, on=\"SampleID\", how=\"inner\")\n",
        "    df = pd.merge(df, subj, on=\"SubjectID\", how=\"left\")\n",
        "\n",
        "    # Locate Files\n",
        "    def get_path(fname):\n",
        "        base = str(fname).replace('.mgb', '.fastq')\n",
        "        for d in Config.FASTQ_DIRS:\n",
        "            for ext in ['', '.gz']:\n",
        "                p = os.path.join(d, base + ext)\n",
        "                if os.path.exists(p): return p\n",
        "        return None\n",
        "\n",
        "    print(\"   Mapping file paths...\")\n",
        "    df['filepath'] = df['filename'].apply(get_path)\n",
        "    df_clean = df.dropna(subset=['filepath']).reset_index(drop=True)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(df_clean)} samples with valid metadata & files.\")\n",
        "    return df_clean\n",
        "\n",
        "# [5] VECTORIZATION ENGINES\n",
        "print(\"üß† Loading Genomic Foundation Model (DNABERT-2)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(Config.DENSE_MODEL_ID, trust_remote_code=True).to(device)\n",
        "\n",
        "def generate_dense_embedding(file_path):\n",
        "    \"\"\"Semantic DNA embedding via Transformer\"\"\"\n",
        "    seq = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            next(f) # Skip header\n",
        "            seq = next(f).strip()[:Config.MAX_DNA_LEN]\n",
        "    except:\n",
        "        return np.zeros(768).tolist()\n",
        "\n",
        "    # Fixed truncation to prevent shape errors\n",
        "    inputs = tokenizer(seq, return_tensors=\"pt\", padding=True, truncation=True, max_length=Config.MAX_DNA_LEN).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = torch.mean(outputs[0], dim=1).squeeze().cpu().numpy()\n",
        "    return embedding.tolist()\n",
        "\n",
        "def generate_sparse_embedding(file_path):\n",
        "    \"\"\"K-mer Counting (Microbial Signature)\"\"\"\n",
        "    tmp_prefix = f\"/tmp/{uuid.uuid4()}\"\n",
        "    subprocess.run(f\"kmc -k{Config.KMER_SIZE} -ci1 -fm {file_path} {tmp_prefix} /tmp\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    subprocess.run(f\"kmc_tools transform {tmp_prefix} dump {tmp_prefix}.txt\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    indices, values = [], []\n",
        "    if os.path.exists(f\"{tmp_prefix}.txt\"):\n",
        "        with open(f\"{tmp_prefix}.txt\", 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    indices.append(hash(parts[0]) % 100000)\n",
        "                    values.append(int(parts[1]))\n",
        "        os.remove(f\"{tmp_prefix}.txt\")\n",
        "        for ext in ['.kmc_pre', '.kmc_suf']:\n",
        "            if os.path.exists(tmp_prefix + ext): os.remove(tmp_prefix + ext)\n",
        "    return indices, values\n",
        "\n",
        "# [6] EXECUTION PIPELINE\n",
        "df = load_metadata()\n",
        "vectors_payload = []\n",
        "\n",
        "print(f\"üöÄ Processing FULL Dataset ({len(df)} samples)...\")\n",
        "\n",
        "# Processing Loop\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    # 1. Generate Vectors\n",
        "    dense = generate_dense_embedding(row['filepath'])\n",
        "    sp_ind, sp_val = generate_sparse_embedding(row['filepath'])\n",
        "\n",
        "    # 2. Structure Data\n",
        "    item = {\n",
        "        \"id\": idx,\n",
        "        \"vector\": {\n",
        "            \"dense\": dense,\n",
        "            \"sparse\": {\"indices\": sp_ind, \"values\": sp_val}\n",
        "        },\n",
        "        \"payload\": {\n",
        "            \"SampleID\": row['SampleID'],\n",
        "            \"BodySite\": row.get('SampleType', 'Unknown'),\n",
        "            # Fixed Key Name for Dashboard Compatibility\n",
        "            \"InsulinSensitivity\": row.get('Insulin_Sensitivity_Label', 'Unknown'),\n",
        "            # Cytokines\n",
        "            \"IL22\": row.get('IL22', 0),\n",
        "            \"EGF\": row.get('EGF', 0),\n",
        "            \"TNFA\": row.get('TNFA', 0)\n",
        "        }\n",
        "    }\n",
        "    vectors_payload.append(item)\n",
        "\n",
        "# [7] EXPORT\n",
        "print(f\"üíæ Saving Bio-Memory to {Config.OUTPUT_FILENAME}...\")\n",
        "with open(Config.OUTPUT_FILENAME, 'wb') as f:\n",
        "    pickle.dump(vectors_payload, f)\n",
        "\n",
        "print(f\"‚úÖ Success! Go to the 'Output' tab to download '{Config.OUTPUT_FILENAME}'.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-19T21:26:53.136743Z",
          "iopub.execute_input": "2026-01-19T21:26:53.137323Z",
          "iopub.status.idle": "2026-01-19T21:43:49.105709Z",
          "shell.execute_reply.started": "2026-01-19T21:26:53.13729Z",
          "shell.execute_reply": "2026-01-19T21:43:49.104864Z"
        },
        "id": "JDx8uocfIjEE",
        "outputId": "a3defb86-1d71-49ae-f0b1-56084c401c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "‚öôÔ∏è Hardware Acceleration: cuda\nüß† Loading Genomic Foundation Model (DNABERT-2)...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT_hyphen_2_hyphen_117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üìÇ Loading Metadata...\n   üßπ Normalizing IDs...\n   Mapping file paths...\n‚úÖ Loaded 1982 samples with valid metadata & files.\nüöÄ Processing FULL Dataset (1982 samples)...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982/1982 [16:51<00:00,  1.96it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "üíæ Saving Bio-Memory to bio_memory_dump.pkl...\n‚úÖ Success! Go to the 'Output' tab to download 'bio_memory_dump.pkl'.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "nRx22eYMIjEF"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}